{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task - 1 - Natural Language Inference (NLI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Install Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T17:18:51.401127Z",
     "iopub.status.busy": "2025-04-01T17:18:51.400844Z",
     "iopub.status.idle": "2025-04-01T17:18:58.926774Z",
     "shell.execute_reply": "2025-04-01T17:18:58.926003Z",
     "shell.execute_reply.started": "2025-04-01T17:18:51.401104Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q peft transformers accelerate bitsandbytes datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Preprocessing the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T17:19:07.050516Z",
     "iopub.status.busy": "2025-04-01T17:19:07.050228Z",
     "iopub.status.idle": "2025-04-01T17:19:25.623359Z",
     "shell.execute_reply": "2025-04-01T17:19:25.622377Z",
     "shell.execute_reply.started": "2025-04-01T17:19:07.050494Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_sentence(parse_str):\n",
    "    return ' '.join(re.findall(r'\\b\\w+\\b', str(parse_str)))\n",
    "\n",
    "def preprocess_dataset(path):\n",
    "    df = pd.read_csv(path, sep='\\t')\n",
    "    df = df.dropna(subset=['Label'])  # ‚úÖ Drop rows with missing labels\n",
    "    df['premise'] = df['Sent1_parse'].apply(extract_sentence)\n",
    "    df['hypothesis'] = df['Sent2_parse'].apply(extract_sentence)\n",
    "    return df[['premise', 'hypothesis', 'Label']] \n",
    "\n",
    "# Set your paths here\n",
    "train_path = '/kaggle/input/recognizing-textual-entailment-rte/train.tsv'\n",
    "val_path = '/kaggle/input/recognizing-textual-entailment-rte/dev.tsv'\n",
    "test_path = '/kaggle/input/recognizing-textual-entailment-rte/test.tsv'\n",
    "\n",
    "# Preprocess\n",
    "train_df = preprocess_dataset(train_path)\n",
    "val_df = preprocess_dataset(val_path)\n",
    "test_df = preprocess_dataset(test_path)\n",
    "\n",
    "# Define valid labels\n",
    "label2id = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "valid_labels = set(label2id.keys())\n",
    "\n",
    "# Drop rows with invalid or missing labels\n",
    "train_df = train_df[train_df['Label'].isin(valid_labels)].copy()\n",
    "val_df = val_df[val_df['Label'].isin(valid_labels)].copy()\n",
    "test_df = test_df[test_df['Label'].isin(valid_labels)].copy()\n",
    "\n",
    "# Now map safely and cast to int\n",
    "train_df['label'] = train_df['Label'].map(label2id).astype(int)\n",
    "val_df['label'] = val_df['Label'].map(label2id).astype(int)\n",
    "test_df['label'] = test_df['Label'].map(label2id).astype(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T19:27:01.633458Z",
     "iopub.status.busy": "2025-04-01T19:27:01.633111Z",
     "iopub.status.idle": "2025-04-01T19:27:01.645098Z",
     "shell.execute_reply": "2025-04-01T19:27:01.644155Z",
     "shell.execute_reply.started": "2025-04-01T19:27:01.633433Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0]\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df['label'].unique())  # should print [0 1 2]\n",
    "print(train_df['label'].dtype)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization & Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T17:19:25.624932Z",
     "iopub.status.busy": "2025-04-01T17:19:25.624544Z",
     "iopub.status.idle": "2025-04-01T17:23:03.622630Z",
     "shell.execute_reply": "2025-04-01T17:23:03.621876Z",
     "shell.execute_reply.started": "2025-04-01T17:19:25.624902Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b6e04c57cf4e0bb52afad74dfa1e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8ef07a0536405a94fde9376d7c700a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd210f79d4d4994b86e810ae3d6dc5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8cb7e06cf841c591909f49765ad19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71130faebd4048ea9fdb2930df14c16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/549367 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0afd9ab5548409da6c560b28be81862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9842 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da6477d3b37437faa0a138a45daa742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9824 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[['premise', 'hypothesis', 'label']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['premise', 'hypothesis', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['premise', 'hypothesis', 'label']])\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"premise\"], example[\"hypothesis\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tuning the hyperparameters using a Hybrid approach of sequential tuning and BOHB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T17:24:49.687499Z",
     "iopub.status.busy": "2025-04-01T17:24:49.687153Z",
     "iopub.status.idle": "2025-04-01T18:55:53.030939Z",
     "shell.execute_reply": "2025-04-01T18:55:53.029994Z",
     "shell.execute_reply.started": "2025-04-01T17:24:49.687477Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîÑ Starting hybrid hyperparameter tuning\n",
      "============================================================\n",
      "============================================================\n",
      "üîç Phase 1: Sequential exploration\n",
      "============================================================\n",
      "GPU memory - Allocated: 0.00 MB, Reserved: 0.00 MB\n",
      "GPU memory - Allocated: 0.00 MB, Reserved: 0.00 MB\n",
      "\n",
      "üîÑ Sequential Trial 1/3\n",
      "Parameters: {'learning_rate': 5e-05, 'batch_size': 32, 'num_epochs': 1, 'weight_decay': 0.01}\n",
      "Using 27468 examples (5.0% of data)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7964e613844f649717007a8b8156d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on CUDA: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "<ipython-input-4-6eee85b208c8>:114: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='430' max='430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [430/430 26:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.639000</td>\n",
       "      <td>0.571431</td>\n",
       "      <td>0.770575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='154' max='154' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [154/154 03:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7706, Time: 1619.0s\n",
      "GPU memory - Allocated: 1298.25 MB, Reserved: 1824.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory - Allocated: 1298.25 MB, Reserved: 1824.00 MB\n",
      "\n",
      "üîÑ Sequential Trial 2/3\n",
      "Parameters: {'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 1, 'weight_decay': 0.01}\n",
      "Using 27468 examples (5.0% of data)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on CUDA: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-6eee85b208c8>:114: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='430' max='430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [430/430 27:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.784700</td>\n",
       "      <td>0.710808</td>\n",
       "      <td>0.701890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='154' max='154' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [154/154 03:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7019, Time: 1630.6s\n",
      "GPU memory - Allocated: 1297.30 MB, Reserved: 2180.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory - Allocated: 1297.30 MB, Reserved: 2180.00 MB\n",
      "\n",
      "üîÑ Sequential Trial 3/3\n",
      "Parameters: {'learning_rate': 3e-05, 'batch_size': 64, 'num_epochs': 1, 'weight_decay': 0.01}\n",
      "Using 27468 examples (5.0% of data)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "<ipython-input-4-6eee85b208c8>:114: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on CUDA: True\n",
      "‚ùå Error in trial 2: Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 1668, in forward\n",
      "    outputs = self.bert(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 1142, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 695, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 585, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 515, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 408, in forward\n",
      "    key_layer = self.transpose_for_scores(self.key(current_states))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 36.12 MiB is free. Process 3488 has 14.70 GiB memory in use. Of the allocated memory 14.44 GiB is allocated by PyTorch, and 67.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "GPU memory - Allocated: 441.25 MB, Reserved: 1148.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-01 18:25:35,500] A new study created in memory with name: bohb_refined_optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sequential Phase Results ===\n",
      "1. Accuracy: 0.7706, LR: 5e-05, BS: 32\n",
      "2. Accuracy: 0.7019, LR: 1e-05, BS: 32\n",
      "============================================================\n",
      "üöÄ Phase 2: BOHB optimization with refined parameter ranges\n",
      "============================================================\n",
      "Learning rate range: 0.000010 to 0.000050\n",
      "Batch sizes: [32]\n",
      "Weight decay range: 0.0000 to 0.1000\n",
      "GPU memory - Allocated: 16.40 MB, Reserved: 68.00 MB\n",
      "\n",
      "üéØ Starting BOHB Trial #0\n",
      "üìä Sampled 27468 examples (5.0% of dataset)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "<ipython-input-4-6eee85b208c8>:261: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='430' max='430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [430/430 27:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.609629</td>\n",
       "      <td>0.752794</td>\n",
       "      <td>0.751901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training completed in 1631.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='154' max='154' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [154/154 03:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-01 18:55:53,023] Trial 0 finished with value: 0.7527941475309896 and parameters: {'learning_rate': 1.827226177606625e-05, 'batch_size': 32, 'weight_decay': 0.09507143064099162}. Best is trial 0 with value: 0.7527941475309896.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory - Allocated: 1299.63 MB, Reserved: 1832.00 MB\n",
      "\n",
      "üéØ BOHB Phase Best Trial\n",
      "Trial #0\n",
      "Accuracy: 0.7528\n",
      "Best Hyperparameters:\n",
      "  learning_rate: 1.827226177606625e-05\n",
      "  batch_size: 32\n",
      "  weight_decay: 0.09507143064099162\n",
      "‚è±Ô∏è Optimization Time: 1817.52 seconds\n",
      "\n",
      "=== Final Hyperparameter Tuning Results ===\n",
      "Best parameters: {'learning_rate': 1.827226177606625e-05, 'batch_size': 32, 'weight_decay': 0.09507143064099162, 'num_epochs': 2}\n",
      "\n",
      "Final best parameters to use for full model training:\n",
      "  learning_rate: 1.827226177606625e-05\n",
      "  batch_size: 32\n",
      "  weight_decay: 0.09507143064099162\n",
      "  num_epochs: 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from functools import partial\n",
    "import optuna\n",
    "from optuna.pruners import HyperbandPruner\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "def force_gpu_cleanup():\n",
    "    \"\"\"Force aggressive GPU memory cleanup\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # Print memory stats for monitoring\n",
    "        allocated = torch.cuda.memory_allocated() / (1024 ** 2)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024 ** 2)\n",
    "        print(f\"GPU memory - Allocated: {allocated:.2f} MB, Reserved: {reserved:.2f} MB\")\n",
    "\n",
    "def run_sequential_phase(train_dataset, val_dataset, tokenizer, id2label, label2id, n_trials=3): \n",
    "    \"\"\"\n",
    "    Run initial sequential hyperparameter exploration to find stable, promising regions\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üîç Phase 1: Sequential exploration\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Force GPU cleanup\n",
    "    force_gpu_cleanup()\n",
    "    \n",
    "    \n",
    "    lr_options = [1e-5, 3e-5, 5e-5]\n",
    "    batch_options = [16, 32, 64]\n",
    "    epochs_options = [1, 2]\n",
    "    wd_options = [0.0, 0.01, 0.1]\n",
    "    \n",
    "    # Create parameter combinations\n",
    "    params_list = []\n",
    "    for lr in lr_options:\n",
    "        for bs in batch_options:\n",
    "            \n",
    "            params_list.append({\n",
    "                \"learning_rate\": lr,\n",
    "                \"batch_size\": bs,\n",
    "                \"num_epochs\": 1,\n",
    "                \"weight_decay\": 0.01\n",
    "            })\n",
    "    \n",
    "    \n",
    "    np.random.seed(42)\n",
    "    if len(params_list) > n_trials:\n",
    "        params_list = list(np.random.choice(params_list, n_trials, replace=False))\n",
    "    \n",
    "    # Track results\n",
    "    results = []\n",
    "    \n",
    "    # Run each trial sequentially\n",
    "    for trial_idx, params in enumerate(params_list):\n",
    "        \n",
    "        force_gpu_cleanup()\n",
    "        \n",
    "        print(f\"\\nüîÑ Sequential Trial {trial_idx+1}/{len(params_list)}\")\n",
    "        print(f\"Parameters: {params}\")\n",
    "        \n",
    "        # Small sample for quick exploration (5%)\n",
    "        sample_ratio = 0.05\n",
    "        sample_size = int(len(train_dataset) * sample_ratio)\n",
    "        indices = np.random.choice(len(train_dataset), sample_size, replace=False)\n",
    "        train_subset = train_dataset.select(indices)\n",
    "        print(f\"Using {len(train_subset)} examples ({sample_ratio*100:.1f}% of data)\")\n",
    "        \n",
    "        try:\n",
    "            # Create model\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                \"bert-base-uncased\",  \n",
    "                num_labels=len(id2label),\n",
    "                id2label=id2label,\n",
    "                label2id=label2id\n",
    "            )\n",
    "            \n",
    "            # Move to GPU\n",
    "            if torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "                print(f\"Model on CUDA: {next(model.parameters()).is_cuda}\")\n",
    "            \n",
    "            # Define metric\n",
    "            def compute_metrics(eval_pred):\n",
    "                logits, labels = eval_pred\n",
    "                preds = np.argmax(logits, axis=-1)\n",
    "                acc = accuracy_score(labels, preds)\n",
    "                return {\"accuracy\": acc}\n",
    "            \n",
    "            # Training args\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=f\"/content/seq_trial_{trial_idx}\",\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                save_strategy=\"no\",\n",
    "                num_train_epochs=params[\"num_epochs\"],\n",
    "                per_device_train_batch_size=params[\"batch_size\"],\n",
    "                per_device_eval_batch_size=params[\"batch_size\"],\n",
    "                learning_rate=params[\"learning_rate\"],\n",
    "                weight_decay=params[\"weight_decay\"],\n",
    "                report_to=\"none\",\n",
    "                fp16=False,  # Disable mixed precision for stability\n",
    "                dataloader_num_workers=0,\n",
    "                logging_steps=100\n",
    "            )\n",
    "            \n",
    "            # Create trainer\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                tokenizer=tokenizer,\n",
    "                train_dataset=train_subset,\n",
    "                eval_dataset=val_dataset,\n",
    "                compute_metrics=compute_metrics\n",
    "            )\n",
    "            \n",
    "            # Train\n",
    "            start_time = time.time()\n",
    "            trainer.train()\n",
    "            train_time = time.time() - start_time\n",
    "            \n",
    "            # Evaluate\n",
    "            eval_result = trainer.evaluate()\n",
    "            accuracy = eval_result[\"eval_accuracy\"]\n",
    "            \n",
    "            print(f\"Accuracy: {accuracy:.4f}, Time: {train_time:.1f}s\")\n",
    "            \n",
    "            # Store result\n",
    "            results.append({\n",
    "                \"params\": params,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"time\": train_time\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in trial {trial_idx}: {str(e)}\")\n",
    "        finally:\n",
    "            # Clean up\n",
    "            if 'model' in locals():\n",
    "                del model\n",
    "            if 'trainer' in locals():\n",
    "                del trainer\n",
    "            force_gpu_cleanup()\n",
    "            time.sleep(1)  \n",
    "    \n",
    "    # Sort results by accuracy\n",
    "    results.sort(key=lambda x: x[\"accuracy\"], reverse=True)\n",
    "    \n",
    "    print(\"\\n=== Sequential Phase Results ===\")\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"{i+1}. Accuracy: {result['accuracy']:.4f}, LR: {result['params']['learning_rate']}, BS: {result['params']['batch_size']}\")\n",
    "    \n",
    "    # Extract best parameter ranges for BOHB\n",
    "    if len(results) > 0:\n",
    "        # Get the top half of results or at least 2\n",
    "        top_count = max(len(results) // 2, min(2, len(results)))\n",
    "        top_results = results[:top_count]\n",
    "        \n",
    "        # Extract parameter ranges\n",
    "        lr_values = [r[\"params\"][\"learning_rate\"] for r in top_results]\n",
    "        bs_values = [r[\"params\"][\"batch_size\"] for r in top_results]\n",
    "        \n",
    "        # Determine ranges\n",
    "        lr_low, lr_high = min(lr_values), max(lr_values)\n",
    "        bs_options = list(set(bs_values))  # Unique batch sizes\n",
    "        \n",
    "        # If all same value, expand slightly\n",
    "        if lr_low == lr_high:\n",
    "            lr_low = lr_low * 0.8\n",
    "            lr_high = lr_high * 1.2\n",
    "            \n",
    "        bohb_ranges = {\n",
    "            \"lr_range\": (lr_low, lr_high),\n",
    "            \"batch_sizes\": bs_options,\n",
    "            \"weight_decay_range\": (0.0, 0.1)\n",
    "        }\n",
    "        \n",
    "        return bohb_ranges, results\n",
    "    else:\n",
    "        # Default ranges if no successful trials\n",
    "        return {\n",
    "            \"lr_range\": (1e-5, 5e-5),\n",
    "            \"batch_sizes\": [16, 32],\n",
    "            \"weight_decay_range\": (0.0, 0.1)\n",
    "        }, []\n",
    "\n",
    "def bohb_objective(trial, train_dataset, val_dataset, tokenizer, id2label, label2id, param_ranges):\n",
    "    \"\"\"Objective function for BOHB optimization with refined parameter ranges\"\"\"\n",
    "    # Force GPU cleanup\n",
    "    force_gpu_cleanup()\n",
    "    \n",
    "    print(f\"\\nüéØ Starting BOHB Trial #{trial.number}\")\n",
    "    \n",
    "    # Get parameter ranges from sequential phase\n",
    "    lr_range = param_ranges[\"lr_range\"]\n",
    "    batch_sizes = param_ranges[\"batch_sizes\"]\n",
    "    wd_range = param_ranges[\"weight_decay_range\"]\n",
    "    \n",
    "    # Suggest hyperparameters within refined ranges\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", lr_range[0], lr_range[1], log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", batch_sizes)\n",
    "    num_epochs = 1  # Fixed to 1 epoch for all BOHB trials\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", wd_range[0], wd_range[1])\n",
    "    \n",
    "    # Progressive sampling\n",
    "    if trial.number < 2:\n",
    "        sample_ratio = 0.05  # 5% for first 2 trials\n",
    "    else:\n",
    "        sample_ratio = 0.10  # 10% for remaining trials\n",
    "    \n",
    "    train_subset_size = int(sample_ratio * len(train_dataset))\n",
    "    \n",
    "    # Simple random sampling\n",
    "    indices = np.random.choice(len(train_dataset), train_subset_size, replace=False)\n",
    "    train_subset = train_dataset.select(indices)\n",
    "    print(f\"üìä Sampled {len(train_subset)} examples ({sample_ratio*100:.1f}% of dataset)\")\n",
    "    \n",
    "    # Define metrics function\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        f1 = f1_score(labels, preds, average=\"macro\")\n",
    "        return {\"accuracy\": acc, \"f1_macro\": f1}\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"/content/bohb_trial_{trial.number}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        report_to=\"none\",\n",
    "        fp16=False, \n",
    "        dataloader_num_workers=0,\n",
    "        dataloader_pin_memory=True\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Create model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"bert-base-uncased\",  \n",
    "            num_labels=len(id2label),\n",
    "            id2label=id2label,\n",
    "            label2id=label2id\n",
    "        )\n",
    "        \n",
    "        # Move to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            tokenizer=tokenizer,\n",
    "            train_dataset=train_subset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        start_time = time.time()\n",
    "        trainer.train()\n",
    "        train_time = time.time() - start_time\n",
    "        print(f\"‚úÖ Training completed in {train_time:.2f} seconds\")\n",
    "        \n",
    "        # Evaluate\n",
    "        metrics = trainer.evaluate()\n",
    "        acc = metrics[\"eval_accuracy\"]\n",
    "        \n",
    "        # Report accuracy for pruning\n",
    "        trial.report(acc, step=num_epochs)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        return acc\n",
    "        \n",
    "    except optuna.exceptions.TrialPruned:\n",
    "        print(f\"Trial #{trial.number} pruned.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå BOHB Trial {trial.number} failed: {str(e)}\")\n",
    "        raise e\n",
    "    finally:\n",
    "        # Clean up\n",
    "        if 'model' in locals():\n",
    "            del model\n",
    "        if 'trainer' in locals():\n",
    "            del trainer\n",
    "        force_gpu_cleanup()\n",
    "\n",
    "def run_bohb_phase(train_dataset, val_dataset, tokenizer, id2label, label2id, param_ranges, n_trials=4): \n",
    "    \"\"\"Run BOHB phase with refined parameter ranges\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üöÄ Phase 2: BOHB optimization with refined parameter ranges\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Print refined ranges\n",
    "    print(f\"Learning rate range: {param_ranges['lr_range'][0]:.6f} to {param_ranges['lr_range'][1]:.6f}\")\n",
    "    print(f\"Batch sizes: {param_ranges['batch_sizes']}\")\n",
    "    print(f\"Weight decay range: {param_ranges['weight_decay_range'][0]:.4f} to {param_ranges['weight_decay_range'][1]:.4f}\")\n",
    "    \n",
    "    # Setup pruner and sampler\n",
    "    pruner = HyperbandPruner(min_resource=1, max_resource=1, reduction_factor=3)  \n",
    "    sampler = TPESampler(n_startup_trials=1, seed=42)\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=sampler,\n",
    "        pruner=pruner,\n",
    "        study_name=\"bohb_refined_optimization\"\n",
    "    )\n",
    "    \n",
    "    # Create objective function with parameter ranges\n",
    "    objective_func = partial(\n",
    "        bohb_objective,\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        param_ranges=param_ranges\n",
    "    )\n",
    "    \n",
    "    # Run optimization\n",
    "    start = time.time()\n",
    "    try:\n",
    "        study.optimize(\n",
    "            objective_func, \n",
    "            n_trials=n_trials,\n",
    "            timeout=1800,  \n",
    "            catch=(Exception,)\n",
    "        )\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Optimization stopped by user.\")\n",
    "    \n",
    "    duration = time.time() - start\n",
    "    \n",
    "    # Print results\n",
    "    if len(study.trials) > 0:\n",
    "        print(\"\\nüéØ BOHB Phase Best Trial\")\n",
    "        print(f\"Trial #{study.best_trial.number}\")\n",
    "        print(f\"Accuracy: {study.best_value:.4f}\")\n",
    "        print(\"Best Hyperparameters:\")\n",
    "        for k, v in study.best_trial.params.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "        print(f\"‚è±Ô∏è Optimization Time: {duration:.2f} seconds\")\n",
    "        \n",
    "        return study.best_trial.params\n",
    "    else:\n",
    "        print(\"No successful BOHB trials\")\n",
    "        return None\n",
    "\n",
    "def run_hybrid_tuning(train_dataset, val_dataset, tokenizer, id2label, label2id):\n",
    "    \"\"\"Run the complete hybrid tuning process\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üîÑ Starting hybrid hyperparameter tuning\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Phase 1: Sequential tuning to find stable parameter ranges\n",
    "    param_ranges, seq_results = run_sequential_phase(\n",
    "        train_dataset, \n",
    "        val_dataset, \n",
    "        tokenizer, \n",
    "        id2label, \n",
    "        label2id, \n",
    "        n_trials=3  \n",
    "    )\n",
    "    \n",
    "    \n",
    "    if len(seq_results) == 0:\n",
    "        print(\"‚ùå No successful sequential trials. Using default parameters.\")\n",
    "        return {\n",
    "            \"learning_rate\": 2e-5,\n",
    "            \"batch_size\": 16,\n",
    "            \"num_epochs\": 2,\n",
    "            \"weight_decay\": 0.01\n",
    "        }\n",
    "    \n",
    "    # Phase 2: BOHB within refined parameter ranges\n",
    "    bohb_params = run_bohb_phase(\n",
    "        train_dataset, \n",
    "        val_dataset, \n",
    "        tokenizer, \n",
    "        id2label, \n",
    "        label2id, \n",
    "        param_ranges,\n",
    "        n_trials=4 \n",
    "    )\n",
    "    \n",
    "    # If BOHB fails, use best from sequential phase\n",
    "    if bohb_params is None:\n",
    "        print(\"Using best parameters from sequential phase\")\n",
    "        best_params = seq_results[0][\"params\"]\n",
    "    else:\n",
    "        best_params = bohb_params\n",
    "        best_params[\"num_epochs\"] = 2  \n",
    "    \n",
    "    print(\"\\n=== Final Hyperparameter Tuning Results ===\")\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "# Execute the hybrid tuning\n",
    "best_params = run_hybrid_tuning(\n",
    "    train_dataset=train_dataset, \n",
    "    val_dataset=val_dataset, \n",
    "    tokenizer=tokenizer, \n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "print(\"\\nFinal best parameters to use for full model training:\")\n",
    "for param_name, param_value in best_params.items():\n",
    "    print(f\"  {param_name}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preparation for fine-tuning(Instruction Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T19:50:21.936848Z",
     "iopub.status.busy": "2025-04-01T19:50:21.936487Z",
     "iopub.status.idle": "2025-04-01T19:50:26.028448Z",
     "shell.execute_reply": "2025-04-01T19:50:26.027779Z",
     "shell.execute_reply.started": "2025-04-01T19:50:21.936819Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìÇ Loading and Preparing Datasets\n",
      "============================================================\n",
      "Loading datasets from TSV files...\n",
      "Loaded datasets with shapes: Train=(550152, 3), Val=(10000, 3), Test=(10000, 3)\n",
      "Train dataframe columns: ['Sent1_parse', 'Sent2_parse', 'Label']\n",
      "First few rows of train data:\n",
      "                                                                                                                                                         Sent1_parse                                                                                                                                          Sent2_parse          Label\n",
      "0  (ROOT (S (NP (NP (DT A) (NN person)) (PP (IN on) (NP (DT a) (NN horse)))) (VP (VBZ jumps) (PP (IN over) (NP (DT a) (JJ broken) (JJ down) (NN airplane)))) (. .)))     (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) (VP (VBG training) (NP (PRP$ his) (NN horse)) (PP (IN for) (NP (DT a) (NN competition))))) (. .)))        neutral\n",
      "1  (ROOT (S (NP (NP (DT A) (NN person)) (PP (IN on) (NP (DT a) (NN horse)))) (VP (VBZ jumps) (PP (IN over) (NP (DT a) (JJ broken) (JJ down) (NN airplane)))) (. .)))  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) (PP (IN at) (NP (DT a) (NN diner))) (, ,) (S (VP (VBG ordering) (NP (DT an) (NN omelette))))) (. .)))  contradiction\n",
      "\n",
      "Checking for missing and placeholder values:\n",
      "Train NaN count: {'Sent1_parse': 0, 'Sent2_parse': 0, 'Label': 0}\n",
      "Val NaN count: {'Sent1_parse': 0, 'Sent2_parse': 0, 'Label': 0}\n",
      "Test NaN count: {'Sent1_parse': 0, 'Sent2_parse': 0, 'Label': 0}\n",
      "Train '-' values in Label column: 785\n",
      "Val '-' values in Label column: 158\n",
      "Test '-' values in Label column: 176\n",
      "\n",
      "Label distribution in train set:\n",
      "Label\n",
      "entailment       183416\n",
      "contradiction    183187\n",
      "neutral          182764\n",
      "-                   785\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Filtering out rows with invalid labels...\n",
      "Filtered train dataset: 549367 rows\n",
      "Filtered validation dataset: 9842 rows\n",
      "Filtered test dataset: 9824 rows\n",
      "\n",
      "Converting to HuggingFace datasets...\n",
      "\n",
      "Final dataset sizes: Train=549367, Val=9842, Test=9824\n",
      "\n",
      "‚úÖ Data preparation complete and ready for model training\n",
      "Available for next stage: train_dataset, val_dataset, test_dataset, and dataframes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datasets import Dataset\n",
    "\n",
    "# Define function to extract plain text from parse trees\n",
    "def extract_sentence(parse_str):\n",
    "    \"\"\"Extract plain text from parse tree\"\"\"\n",
    "    return ' '.join(re.findall(r'\\b\\w+\\b', str(parse_str)))\n",
    "\n",
    "def load_datasets():\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìÇ Loading and Preparing Datasets\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    \n",
    "    print(\"Loading datasets from TSV files...\")\n",
    "    try:\n",
    "        train_df = pd.read_csv('/kaggle/input/recognizing-textual-entailment-rte/train.tsv', sep='\\t')\n",
    "        val_df = pd.read_csv('/kaggle/input/recognizing-textual-entailment-rte/dev.tsv', sep='\\t')\n",
    "        test_df = pd.read_csv('/kaggle/input/recognizing-textual-entailment-rte/test.tsv', sep='\\t')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading from Kaggle path: {e}\")\n",
    "        # Try alternative paths\n",
    "        try:\n",
    "            train_df = pd.read_csv('./train.tsv', sep='\\t')\n",
    "            val_df = pd.read_csv('./dev.tsv', sep='\\t')\n",
    "            test_df = pd.read_csv('./test.tsv', sep='\\t')\n",
    "        except Exception as e2:\n",
    "            print(f\"Error with alternative path: {e2}\")\n",
    "            raise\n",
    "\n",
    "    print(f\"Loaded datasets with shapes: Train={train_df.shape}, Val={val_df.shape}, Test={test_df.shape}\")\n",
    "    \n",
    "    # Examine dataset columns\n",
    "    print(f\"Train dataframe columns: {train_df.columns.tolist()}\")\n",
    "    print(f\"First few rows of train data:\")\n",
    "    print(train_df.head(2).to_string())\n",
    "    \n",
    "    # Check for NaN values and \"-\" placeholder values\n",
    "    print(\"\\nChecking for missing and placeholder values:\")\n",
    "    print(\"Train NaN count:\", train_df.isna().sum().to_dict())\n",
    "    print(\"Val NaN count:\", val_df.isna().sum().to_dict())\n",
    "    print(\"Test NaN count:\", test_df.isna().sum().to_dict())\n",
    "    \n",
    "    # Check for \"-\" values in the Label column\n",
    "    if 'Label' in train_df.columns:\n",
    "        train_dash_count = (train_df['Label'] == \"-\").sum()\n",
    "        print(f\"Train '-' values in Label column: {train_dash_count}\")\n",
    "    if 'Label' in val_df.columns:\n",
    "        val_dash_count = (val_df['Label'] == \"-\").sum()\n",
    "        print(f\"Val '-' values in Label column: {val_dash_count}\")\n",
    "    if 'Label' in test_df.columns:\n",
    "        test_dash_count = (test_df['Label'] == \"-\").sum()\n",
    "        print(f\"Test '-' values in Label column: {test_dash_count}\")\n",
    "    \n",
    "    # Check label distribution\n",
    "    if 'Label' in train_df.columns:\n",
    "        print(\"\\nLabel distribution in train set:\")\n",
    "        print(train_df['Label'].value_counts())\n",
    "    \n",
    "    # Define label mappings\n",
    "    label2id = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "    \n",
    "    # Filter out rows with \"-\" as the label\n",
    "    print(\"\\nFiltering out rows with invalid labels...\")\n",
    "    if 'Label' in train_df.columns:\n",
    "        train_df = train_df[train_df['Label'] != \"-\"].reset_index(drop=True)\n",
    "        print(f\"Filtered train dataset: {len(train_df)} rows\")\n",
    "    \n",
    "    if 'Label' in val_df.columns:\n",
    "        val_df = val_df[val_df['Label'] != \"-\"].reset_index(drop=True)\n",
    "        print(f\"Filtered validation dataset: {len(val_df)} rows\")\n",
    "    \n",
    "    if 'Label' in test_df.columns:\n",
    "        test_df = test_df[test_df['Label'] != \"-\"].reset_index(drop=True)\n",
    "        print(f\"Filtered test dataset: {len(test_df)} rows\")\n",
    "        \n",
    "    # Convert to HuggingFace datasets\n",
    "    print(\"\\nConverting to HuggingFace datasets...\")\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset = Dataset.from_pandas(val_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    # Sample data for faster training if needed\n",
    "    sample_ratio = 1.0  # Set to < 1.0 for sampling\n",
    "    if sample_ratio < 1.0:\n",
    "        print(f\"Sampling {sample_ratio*100:.1f}% of training data for faster development...\")\n",
    "        train_size = int(len(train_dataset) * sample_ratio)\n",
    "        indices = np.random.choice(len(train_dataset), train_size, replace=False)\n",
    "        train_dataset = train_dataset.select(indices)\n",
    "        print(f\"Sampled train size: {len(train_dataset)}\")\n",
    "    \n",
    "    print(f\"\\nFinal dataset sizes: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset, train_df, val_df, test_df, label2id, id2label\n",
    "\n",
    "# Execute data preparation\n",
    "if __name__ == \"__main__\":\n",
    "    train_dataset, val_dataset, test_dataset, train_df, val_df, test_df, label2id, id2label = load_datasets()\n",
    "    \n",
    "    print(\"\\n‚úÖ Data preparation complete and ready for model training\")\n",
    "    print(f\"Available for next stage: train_dataset, val_dataset, test_dataset, and dataframes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Model Training Using Best Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T19:54:19.361853Z",
     "iopub.status.busy": "2025-04-01T19:54:19.361475Z",
     "iopub.status.idle": "2025-04-02T03:28:33.835460Z",
     "shell.execute_reply": "2025-04-02T03:28:33.834673Z",
     "shell.execute_reply.started": "2025-04-01T19:54:19.361824Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üöÄ Training NLI model with instruction tuning\n",
      "============================================================\n",
      "‚úÖ Using GPU: Tesla T4\n",
      "üìä Training Configuration:\n",
      "  Learning Rate: 1.827226177606625e-05\n",
      "  Batch Size: 64\n",
      "  Weight Decay: 0.09507143064099162\n",
      "  Epochs: 2\n",
      "  Train Dataset Size: 549367\n",
      "  Validation Dataset Size: 9842\n",
      "  Test Dataset Size: 9824\n",
      "Model artifacts will be saved to: /kaggle/working/nli_model_20250401_195419\n",
      "Converting datasets to instruction format...\n",
      "Available columns in dataset: ['Sent1_parse', 'Sent2_parse', 'Label']\n",
      "Using 'Sent1_parse' for premise, 'Sent2_parse' for hypothesis, and 'Label' for label\n",
      "Available columns in dataset: ['Sent1_parse', 'Sent2_parse', 'Label']\n",
      "Using 'Sent1_parse' for premise, 'Sent2_parse' for hypothesis, and 'Label' for label\n",
      "Available columns in dataset: ['Sent1_parse', 'Sent2_parse', 'Label']\n",
      "Using 'Sent1_parse' for premise, 'Sent2_parse' for hypothesis, and 'Label' for label\n",
      "Instruction dataset sizes: Train=549367, Val=9842, Test=9824\n",
      "Initializing tokenizer...\n",
      "Tokenizing instruction datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee33fe4600d4ce08b5bd779a3be3ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/549367 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b453a6f103cb4ccb8fc0ef066d259591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9842 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e23653176c44d508b414cf71490f222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9824 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "‚úÖ Model successfully moved to GPU\n",
      "\n",
      "‚è≥ Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-c75878347af3>:234: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2146' max='2146' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2146/2146 7:27:40, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.541500</td>\n",
       "      <td>0.358282</td>\n",
       "      <td>0.866389</td>\n",
       "      <td>0.866571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>0.328452</td>\n",
       "      <td>0.879293</td>\n",
       "      <td>0.879127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training completed in 7h 27m 56s\n",
      "\n",
      "‚è≥ Evaluating on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.3284517228603363, 'eval_accuracy': 0.8792928266612478, 'eval_f1_macro': 0.8791267145023608, 'eval_runtime': 85.8952, 'eval_samples_per_second': 114.581, 'eval_steps_per_second': 0.454, 'epoch': 2.0}\n",
      "\n",
      "‚è≥ Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: {'eval_loss': 0.326068252325058, 'eval_accuracy': 0.8795806188925082, 'eval_f1_macro': 0.879295572962462, 'eval_runtime': 85.4599, 'eval_samples_per_second': 114.955, 'eval_steps_per_second': 0.456, 'epoch': 2.0}\n",
      "‚úÖ Model saved to /kaggle/working/nli_model_20250401_195419/best-model\n",
      "\n",
      "‚úÖ Model training complete!\n",
      "Model saved at: /kaggle/working/nli_model_20250401_195419/best-model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def extract_sentence(parse_str):\n",
    "    \"\"\"Extract plain text from parse tree\"\"\"\n",
    "    return ' '.join(re.findall(r'\\b\\w+\\b', str(parse_str)))\n",
    "\n",
    "def format_instruction_examples(dataset):\n",
    "    \"\"\"Convert dataset examples to instruction format\"\"\"\n",
    "    formatted_dataset = []\n",
    "    \n",
    "    # First check what columns are actually available\n",
    "    if len(dataset) > 0:\n",
    "        first_example = dataset[0]\n",
    "        columns = list(first_example.keys())\n",
    "        print(f\"Available columns in dataset: {columns}\")\n",
    "        \n",
    "        \n",
    "        premise_key = None\n",
    "        hypothesis_key = None\n",
    "        label_key = None\n",
    "        \n",
    "        if 'premise' in columns and 'hypothesis' in columns:\n",
    "            premise_key = 'premise'\n",
    "            hypothesis_key = 'hypothesis'\n",
    "        elif 'Sent1_parse' in columns and 'Sent2_parse' in columns:\n",
    "            premise_key = 'Sent1_parse'\n",
    "            hypothesis_key = 'Sent2_parse'\n",
    "        else:\n",
    "            raise ValueError(f\"Could not find premise and hypothesis columns in dataset. Available columns: {columns}\")\n",
    "            \n",
    "        # Determine label column\n",
    "        if 'label' in columns:\n",
    "            label_key = 'label'\n",
    "        elif 'Label' in columns:\n",
    "            label_key = 'Label'\n",
    "        \n",
    "        print(f\"Using '{premise_key}' for premise, '{hypothesis_key}' for hypothesis, and '{label_key}' for label\")\n",
    "        \n",
    "        # Process each example\n",
    "        for i in range(len(dataset)):\n",
    "            example = dataset[i]\n",
    "            \n",
    "            # Extract premise and hypothesis\n",
    "            if premise_key in example and hypothesis_key in example:\n",
    "                premise_raw = example[premise_key]\n",
    "                hypothesis_raw = example[hypothesis_key]\n",
    "                \n",
    "                # Clean text if needed (remove parse trees if using Sent*_parse columns)\n",
    "                premise = extract_sentence(premise_raw) if premise_key == 'Sent1_parse' else premise_raw\n",
    "                hypothesis = extract_sentence(hypothesis_raw) if hypothesis_key == 'Sent2_parse' else hypothesis_raw\n",
    "                \n",
    "                # Format as instruction\n",
    "                instruction = f\"\"\"\n",
    "Task: Determine if the premise entails, contradicts, or is neutral to the hypothesis.\n",
    "\n",
    "Premise: {premise}\n",
    "Hypothesis: {hypothesis}\n",
    "\n",
    "Answer with only one word: entailment, contradiction, or neutral.\n",
    "\"\"\"\n",
    "                \n",
    "                # For training examples, we have labels\n",
    "                label = None\n",
    "                if label_key and label_key in example:\n",
    "                    label = example[label_key]\n",
    "                    # Convert string label to id if needed\n",
    "                    if isinstance(label, str):\n",
    "                        # Skip invalid labels like \"-\"\n",
    "                        if label == \"-\":\n",
    "                            continue\n",
    "                        label_map = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "                        label = label_map.get(label.lower(), None)\n",
    "                \n",
    "                # Convert tensor to int if needed\n",
    "                if hasattr(label, 'item'):\n",
    "                    label = label.item()\n",
    "                \n",
    "                formatted_dataset.append({\n",
    "                    \"input\": instruction,\n",
    "                    \"label\": label\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Warning: Missing premise or hypothesis in example {i}\")\n",
    "    else:\n",
    "        print(\"Warning: Dataset is empty!\")\n",
    "    \n",
    "    return Dataset.from_list(formatted_dataset)\n",
    "\n",
    "def train_nli_model(train_dataset, val_dataset, test_dataset, label2id, id2label, best_params=None):\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üöÄ Training NLI model with instruction tuning\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Force GPU cleanup before starting\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(f\"‚úÖ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No GPU detected - training may be slow\")\n",
    "    \n",
    "    # Set default hyperparameters if not provided\n",
    "    if best_params is None:\n",
    "        best_params = {\n",
    "            \"learning_rate\": 2e-5,\n",
    "            \"batch_size\": 32,\n",
    "            \"num_epochs\": 3,\n",
    "            \"weight_decay\": 0.01\n",
    "        }\n",
    "    \n",
    "    # Extract hyperparameters\n",
    "    learning_rate = best_params.get(\"learning_rate\", 2e-5)\n",
    "    batch_size = best_params.get(\"batch_size\", 32)\n",
    "    weight_decay = best_params.get(\"weight_decay\", 0.01)\n",
    "    num_epochs = best_params.get(\"num_epochs\", 3)\n",
    "    \n",
    "    print(f\"üìä Training Configuration:\")\n",
    "    print(f\"  Learning Rate: {learning_rate}\")\n",
    "    print(f\"  Batch Size: {batch_size}\")\n",
    "    print(f\"  Weight Decay: {weight_decay}\")\n",
    "    print(f\"  Epochs: {num_epochs}\")\n",
    "    print(f\"  Train Dataset Size: {len(train_dataset)}\")\n",
    "    print(f\"  Validation Dataset Size: {len(val_dataset)}\")\n",
    "    print(f\"  Test Dataset Size: {len(test_dataset)}\")\n",
    "    \n",
    "    # Create timestamp for unique run name\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = f\"/kaggle/working/nli_model_{timestamp}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Model artifacts will be saved to: {output_dir}\")\n",
    "    \n",
    "    # Convert datasets to instruction format\n",
    "    print(\"Converting datasets to instruction format...\")\n",
    "    train_instruction = format_instruction_examples(train_dataset)\n",
    "    val_instruction = format_instruction_examples(val_dataset)\n",
    "    test_instruction = format_instruction_examples(test_dataset)\n",
    "    \n",
    "    print(f\"Instruction dataset sizes: Train={len(train_instruction)}, Val={len(val_instruction)}, Test={len(test_instruction)}\")\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    print(\"Initializing tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    # Create tokenization function for instruction formatted data\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"input\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "    \n",
    "    # Tokenize the datasets\n",
    "    print(\"Tokenizing instruction datasets...\")\n",
    "    train_tokenized = train_instruction.map(tokenize_function, batched=True)\n",
    "    val_tokenized = val_instruction.map(tokenize_function, batched=True)\n",
    "    test_tokenized = test_instruction.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Set format for PyTorch\n",
    "    train_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    val_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    test_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    \n",
    "    # Define metrics calculation function\n",
    "    def compute_metrics(eval_pred):\n",
    "        from sklearn.metrics import accuracy_score, f1_score\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        f1 = f1_score(labels, preds, average=\"macro\")\n",
    "        return {\n",
    "            \"accuracy\": acc,\n",
    "            \"f1_macro\": f1\n",
    "        }\n",
    "    \n",
    "    # Training arguments for final model\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=1000,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size * 2,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        gradient_accumulation_steps=4,\n",
    "        greater_is_better=True,\n",
    "        report_to=\"none\",\n",
    "        fp16=True if torch.cuda.is_available() else False,\n",
    "        save_total_limit=1,\n",
    "        dataloader_num_workers=0,\n",
    "        dataloader_pin_memory=True,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    print(\"Creating model...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels=len(id2label),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "    \n",
    "    # Explicitly move model to GPU\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        print(f\"‚úÖ Model successfully moved to GPU\")\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=2,\n",
    "        early_stopping_threshold=0.001\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_tokenized,\n",
    "        eval_dataset=val_tokenized,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[early_stopping_callback],\n",
    "        data_collator=DataCollatorWithPadding(tokenizer)\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\n‚è≥ Training the model...\")\n",
    "    start_time = time.time()\n",
    "    train_result = trainer.train()\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    hours = int(training_time // 3600)\n",
    "    minutes = int((training_time % 3600) // 60)\n",
    "    seconds = int(training_time % 60)\n",
    "    print(f\"‚úÖ Training completed in {hours}h {minutes}m {seconds}s\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(\"\\n‚è≥ Evaluating on validation set...\")\n",
    "    val_metrics = trainer.evaluate(val_tokenized)\n",
    "    print(f\"Validation metrics: {val_metrics}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\n‚è≥ Evaluating on test set...\")\n",
    "    test_metrics = trainer.evaluate(test_tokenized)\n",
    "    print(f\"Test metrics: {test_metrics}\")\n",
    "    \n",
    "    # Save the final model\n",
    "    best_model_path = f\"{output_dir}/best-model\"\n",
    "    trainer.save_model(best_model_path)\n",
    "    tokenizer.save_pretrained(best_model_path)\n",
    "    print(f\"‚úÖ Model saved to {best_model_path}\")\n",
    "    \n",
    "    # Save model info to a text file\n",
    "    with open(f\"{output_dir}/model_info.txt\", 'w') as f:\n",
    "        f.write(f\"Model trained at: {timestamp}\\n\")\n",
    "        f.write(f\"Parameters: {best_params}\\n\")\n",
    "        f.write(f\"Validation accuracy: {val_metrics['eval_accuracy']:.4f}\\n\")\n",
    "        f.write(f\"Validation F1 macro: {val_metrics['eval_f1_macro']:.4f}\\n\")\n",
    "        f.write(f\"Test accuracy: {test_metrics['eval_accuracy']:.4f}\\n\")\n",
    "        f.write(f\"Test F1 macro: {test_metrics['eval_f1_macro']:.4f}\\n\")\n",
    "    \n",
    "    return model, trainer, tokenizer, val_metrics, test_metrics, best_model_path, output_dir\n",
    "\n",
    "# Execute model training\n",
    "# Best hyperparameters from your tuning\n",
    "best_params = {\n",
    "    \"learning_rate\": 1.827226177606625e-05,  \n",
    "    \"batch_size\": 64, \n",
    "    \"weight_decay\": 0.09507143064099162,  \n",
    "    \"num_epochs\": 2  \n",
    "}\n",
    "\n",
    "# Use the variables from the previous cell\n",
    "# train_dataset, val_dataset, test_dataset should be defined in the previous cell\n",
    "# label2id, id2label should also be defined in the previous cell\n",
    "\n",
    "# Train the model\n",
    "model, trainer, tokenizer, val_metrics, test_metrics, model_path, output_dir = train_nli_model(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    best_params=best_params\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Model training complete!\")\n",
    "print(f\"Model saved at: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.Evaluation and Generating Prediction Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T03:38:55.812232Z",
     "iopub.status.busy": "2025-04-02T03:38:55.811892Z",
     "iopub.status.idle": "2025-04-02T03:46:45.418138Z",
     "shell.execute_reply": "2025-04-02T03:46:45.417402Z",
     "shell.execute_reply.started": "2025-04-02T03:38:55.812209Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä Evaluating Model and Generating Submission Files\n",
      "============================================================\n",
      "\n",
      "Preparing tokenized datasets for evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0622c74ccb8f4a58a0394269e7d8e5a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/549367 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4610e136545c4f138d712bc8f1c308d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9842 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96cf5d90a9e4e24be3ec584d4643be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9824 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset sizes: Train=549367, Val=9842, Test=9824\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è≥ Generating detailed test metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Generating submission files...\n",
      "Processing validation file...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3bdf30e994b4910bc7af257d4ac5a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9842 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Validation submission file saved to /kaggle/working/nli_model_20250401_195419/validation_submission.tsv\n",
      "Processing test file...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3343cff14dfc48a7b507bb1d21d6d504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9824 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test submission file saved to /kaggle/working/nli_model_20250401_195419/test_submission.tsv\n",
      "\n",
      "üîç Validation Prediction Analysis:\n",
      "Accuracy: 0.8793 (on 9842 labeled examples)\n",
      "\n",
      "Prediction distribution:\n",
      "predicted_label\n",
      "contradiction    3318\n",
      "neutral          3263\n",
      "entailment       3261\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Accuracy by class:\n",
      "entailment: 0.8895 (3329 examples)\n",
      "contradiction: 0.8996 (3278 examples)\n",
      "neutral: 0.8482 (3235 examples)\n",
      "\n",
      "Found 0 rows with None values in the validation set Label column.\n",
      "Found 0 rows with '-' values in the validation set Label column.\n",
      "\n",
      "‚úÖ Evaluation complete and submission files generated!\n",
      "Validation file: /kaggle/working/nli_model_20250401_195419/validation_submission.tsv\n",
      "Test file: /kaggle/working/nli_model_20250401_195419/test_submission.tsv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def extract_sentence(parse_str):\n",
    "    \"\"\"Extract plain text from parse tree\"\"\"\n",
    "    return ' '.join(re.findall(r'\\b\\w+\\b', str(parse_str)))\n",
    "\n",
    "def format_instruction_examples(dataset):\n",
    "    \"\"\"Convert dataset examples to instruction format\"\"\"\n",
    "    formatted_dataset = []\n",
    "    \n",
    "    # Process each example\n",
    "    for i in range(len(dataset)):\n",
    "        example = dataset[i]\n",
    "        \n",
    "        if 'Sent1_parse' in example and 'Sent2_parse' in example:\n",
    "            premise_raw = example['Sent1_parse']\n",
    "            hypothesis_raw = example['Sent2_parse']\n",
    "            \n",
    "            premise = extract_sentence(premise_raw)\n",
    "            hypothesis = extract_sentence(hypothesis_raw)\n",
    "            \n",
    "            # Format as instruction\n",
    "            instruction = f\"\"\"\n",
    "Task: Determine if the premise entails, contradicts, or is neutral to the hypothesis.\n",
    "\n",
    "Premise: {premise}\n",
    "Hypothesis: {hypothesis}\n",
    "\n",
    "Answer with only one word: entailment, contradiction, or neutral.\n",
    "\"\"\"\n",
    "            \n",
    "            # For training examples, get label if available\n",
    "            label = None\n",
    "            if 'Label' in example:\n",
    "                label_text = example['Label']\n",
    "                if isinstance(label_text, str) and label_text != \"-\":\n",
    "                    if label_text == 'entailment':\n",
    "                        label = 0\n",
    "                    elif label_text == 'contradiction':\n",
    "                        label = 1\n",
    "                    elif label_text == 'neutral':\n",
    "                        label = 2\n",
    "            \n",
    "            formatted_dataset.append({\n",
    "                \"input\": instruction,\n",
    "                \"label\": label\n",
    "            })\n",
    "    \n",
    "    return Dataset.from_list(formatted_dataset)\n",
    "\n",
    "def prepare_tokenized_datasets(train_dataset, val_dataset, test_dataset):\n",
    "    \"\"\"\n",
    "    Prepare tokenized datasets for evaluation\n",
    "    \"\"\"\n",
    "    print(\"\\nPreparing tokenized datasets for evaluation...\")\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    # Convert to instruction format\n",
    "    train_instruction = format_instruction_examples(train_dataset)\n",
    "    val_instruction = format_instruction_examples(val_dataset)\n",
    "    test_instruction = format_instruction_examples(test_dataset)\n",
    "    \n",
    "    # Create tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"input\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    train_tokenized = train_instruction.map(tokenize_function, batched=True)\n",
    "    val_tokenized = val_instruction.map(tokenize_function, batched=True)\n",
    "    test_tokenized = test_instruction.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Set format for PyTorch\n",
    "    train_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    val_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    test_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    \n",
    "    print(f\"Tokenized dataset sizes: Train={len(train_tokenized)}, Val={len(val_tokenized)}, Test={len(test_tokenized)}\")\n",
    "    \n",
    "    return train_tokenized, val_tokenized, test_tokenized, tokenizer\n",
    "\n",
    "def plot_results(val_metrics, test_metrics, id2label, output_dir):\n",
    "    \"\"\"\n",
    "    Create visualizations of model performance\n",
    "    \"\"\"\n",
    "    # Create bar chart comparing validation and test metrics\n",
    "    metrics = ['accuracy', 'f1_macro']\n",
    "    val_values = [val_metrics['eval_' + m] for m in metrics]\n",
    "    test_values = [test_metrics['eval_' + m] for m in metrics]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, val_values, width, label='Validation')\n",
    "    plt.bar(x + width/2, test_values, width, label='Test')\n",
    "    \n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Model Performance')\n",
    "    plt.xticks(x, ['Accuracy', 'F1 Macro'])\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1.0)\n",
    "    \n",
    "    for i, v in enumerate(val_values):\n",
    "        plt.text(i - width/2, v + 0.01, f'{v:.3f}', ha='center')\n",
    "    \n",
    "    for i, v in enumerate(test_values):\n",
    "        plt.text(i + width/2, v + 0.01, f'{v:.3f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/model_performance.png')\n",
    "    plt.show()\n",
    "\n",
    "def generate_confusion_matrix(trainer, test_tokenized, id2label, output_dir):\n",
    "    \"\"\"\n",
    "    Generate and save confusion matrix visualization\n",
    "    Uses tokenized test dataset\n",
    "    \"\"\"\n",
    "    print(\"\\n‚è≥ Generating detailed test metrics...\")\n",
    "    test_preds = trainer.predict(test_tokenized)\n",
    "    preds = np.argmax(test_preds.predictions, axis=-1)\n",
    "    labels = test_preds.label_ids\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    \n",
    "    \n",
    "    label_names = list(id2label.values())\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=label_names, \n",
    "                yticklabels=label_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    confusion_matrix_path = f\"{output_dir}/confusion_matrix.png\"\n",
    "    plt.savefig(confusion_matrix_path)\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(labels, preds, \n",
    "                                  target_names=label_names, \n",
    "                                  output_dict=True)\n",
    "    \n",
    "    # Save the classification report as a text file\n",
    "    with open(f\"{output_dir}/classification_report.txt\", 'w') as f:\n",
    "        f.write(classification_report(labels, preds, target_names=label_names))\n",
    "    \n",
    "    return report\n",
    "\n",
    "def generate_submission_files(trainer, tokenizer, val_df, test_df, id2label, output_dir):\n",
    "    \"\"\"\n",
    "    Generate prediction files in the required format for submission\n",
    "    Adds predictions as the last column of the original TSV files\n",
    "    \"\"\"\n",
    "    print(\"\\nüìù Generating submission files...\")\n",
    "    \n",
    "    # Process validation file\n",
    "    print(\"Processing validation file...\")\n",
    "    val_examples = []\n",
    "    for idx, row in val_df.iterrows():\n",
    "        premise = extract_sentence(row['Sent1_parse'])\n",
    "        hypothesis = extract_sentence(row['Sent2_parse'])\n",
    "        \n",
    "        instruction = f\"\"\"\n",
    "Task: Determine if the premise entails, contradicts, or is neutral to the hypothesis.\n",
    "\n",
    "Premise: {premise}\n",
    "Hypothesis: {hypothesis}\n",
    "\n",
    "Answer with only one word: entailment, contradiction, or neutral.\n",
    "\"\"\"\n",
    "        val_examples.append({\"input\": instruction})\n",
    "    \n",
    "    val_instruction_dataset = Dataset.from_list(val_examples)\n",
    "    \n",
    "    # Tokenize\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"input\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "    \n",
    "    val_tokenized = val_instruction_dataset.map(tokenize_function, batched=True)\n",
    "    val_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    \n",
    "    # Generate predictions\n",
    "    val_predictions = trainer.predict(val_tokenized)\n",
    "    val_pred_labels = np.argmax(val_predictions.predictions, axis=1)\n",
    "    val_pred_text = [id2label[i] for i in val_pred_labels]\n",
    "    \n",
    "    # Add to original dataframe\n",
    "    val_df_with_preds = val_df.copy()\n",
    "    val_df_with_preds['predicted_label'] = val_pred_text\n",
    "    val_submission_path = f\"{output_dir}/validation_submission.tsv\"\n",
    "    val_df_with_preds.to_csv(val_submission_path, sep='\\t', index=False)\n",
    "    print(f\"‚úÖ Validation submission file saved to {val_submission_path}\")\n",
    "    \n",
    "    # Process test file\n",
    "    print(\"Processing test file...\")\n",
    "    test_examples = []\n",
    "    for idx, row in test_df.iterrows():\n",
    "        premise = extract_sentence(row['Sent1_parse'])\n",
    "        hypothesis = extract_sentence(row['Sent2_parse'])\n",
    "        \n",
    "        instruction = f\"\"\"\n",
    "Task: Determine if the premise entails, contradicts, or is neutral to the hypothesis.\n",
    "\n",
    "Premise: {premise}\n",
    "Hypothesis: {hypothesis}\n",
    "\n",
    "Answer with only one word: entailment, contradiction, or neutral.\n",
    "\"\"\"\n",
    "        test_examples.append({\"input\": instruction})\n",
    "    \n",
    "    test_instruction_dataset = Dataset.from_list(test_examples)\n",
    "    test_tokenized = test_instruction_dataset.map(tokenize_function, batched=True)\n",
    "    test_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    \n",
    "    # Generate predictions\n",
    "    test_predictions = trainer.predict(test_tokenized)\n",
    "    test_pred_labels = np.argmax(test_predictions.predictions, axis=1)\n",
    "    test_pred_text = [id2label[i] for i in test_pred_labels]\n",
    "    \n",
    "    # Add to original dataframe\n",
    "    test_df_with_preds = test_df.copy()\n",
    "    test_df_with_preds['predicted_label'] = test_pred_text\n",
    "    test_submission_path = f\"{output_dir}/test_submission.tsv\"\n",
    "    test_df_with_preds.to_csv(test_submission_path, sep='\\t', index=False)\n",
    "    print(f\"‚úÖ Test submission file saved to {test_submission_path}\")\n",
    "    \n",
    "    return val_df_with_preds, test_df_with_preds\n",
    "\n",
    "def analyze_validation_results(val_df_with_preds):\n",
    "    \"\"\"\n",
    "    Analyze validation prediction results in detail\n",
    "    \"\"\"\n",
    "    print(\"\\nüîç Validation Prediction Analysis:\")\n",
    "    if 'Label' in val_df_with_preds.columns and 'predicted_label' in val_df_with_preds.columns:\n",
    "        # Drop rows with None labels or \"-\" before calculating accuracy\n",
    "        valid_rows = val_df_with_preds.dropna(subset=['Label'])\n",
    "        valid_rows = valid_rows[valid_rows['Label'] != \"-\"]\n",
    "        \n",
    "        if len(valid_rows) > 0:\n",
    "            # Count correct/incorrect predictions\n",
    "            valid_rows['is_correct'] = valid_rows['Label'] == valid_rows['predicted_label']\n",
    "            accuracy = valid_rows['is_correct'].mean()\n",
    "            print(f\"Accuracy: {accuracy:.4f} (on {len(valid_rows)} labeled examples)\")\n",
    "            \n",
    "            # Count predictions by class\n",
    "            print(\"\\nPrediction distribution:\")\n",
    "            print(val_df_with_preds['predicted_label'].value_counts())\n",
    "            \n",
    "            # Check accuracy by class\n",
    "            print(\"\\nAccuracy by class:\")\n",
    "            for label in ['entailment', 'contradiction', 'neutral']:\n",
    "                class_rows = valid_rows[valid_rows['Label'] == label]\n",
    "                if len(class_rows) > 0:\n",
    "                    class_acc = class_rows['is_correct'].mean()\n",
    "                    print(f\"{label}: {class_acc:.4f} ({len(class_rows)} examples)\")\n",
    "        else:\n",
    "            print(\"No valid labeled examples found for evaluation.\")\n",
    "    else:\n",
    "        print(\"Required columns 'Label' or 'predicted_label' not found in validation data.\")\n",
    "\n",
    "    # Count None and \"-\" values in the original dataset\n",
    "    if 'Label' in val_df_with_preds.columns:\n",
    "        none_count = val_df_with_preds['Label'].isna().sum()\n",
    "        dash_count = (val_df_with_preds['Label'] == \"-\").sum()\n",
    "        print(f\"\\nFound {none_count} rows with None values in the validation set Label column.\")\n",
    "        print(f\"Found {dash_count} rows with '-' values in the validation set Label column.\")\n",
    "\n",
    "# Execute the evaluation and submission\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä Evaluating Model and Generating Submission Files\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# We need to recreate the tokenized datasets \n",
    "# First, make sure we have access to the original datasets and trained model\n",
    "train_tokenized, val_tokenized, test_tokenized, new_tokenizer = prepare_tokenized_datasets(\n",
    "    train_dataset, val_dataset, test_dataset\n",
    ")\n",
    "\n",
    "# If tokenizer wasn't saved properly, use the newly created one\n",
    "if 'tokenizer' not in globals():\n",
    "    tokenizer = new_tokenizer\n",
    "\n",
    "# Plot metrics comparison\n",
    "plot_results(val_metrics, test_metrics, id2label, output_dir)\n",
    "\n",
    "# Generate confusion matrix and classification report\n",
    "report = generate_confusion_matrix(trainer, test_tokenized, id2label, output_dir)\n",
    "\n",
    "# Generate submission files\n",
    "val_df_with_preds, test_df_with_preds = generate_submission_files(\n",
    "    trainer, tokenizer, val_df, test_df, id2label, output_dir\n",
    ")\n",
    "\n",
    "# Analyze validation results\n",
    "analyze_validation_results(val_df_with_preds)\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete and submission files generated!\")\n",
    "print(f\"Validation file: {output_dir}/validation_submission.tsv\")\n",
    "print(f\"Test file: {output_dir}/test_submission.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T04:10:56.607093Z",
     "iopub.status.busy": "2025-04-02T04:10:56.606719Z",
     "iopub.status.idle": "2025-04-02T04:12:23.502326Z",
     "shell.execute_reply": "2025-04-02T04:12:23.501506Z",
     "shell.execute_reply.started": "2025-04-02T04:10:56.607059Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/nli_model_20250401_195419/ (stored 0%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/best-model/ (stored 0%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/best-model/config.json (deflated 51%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/best-model/model.safetensors (deflated 7%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/best-model/tokenizer_config.json (deflated 75%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/best-model/tokenizer.json (deflated 71%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/best-model/training_args.bin (deflated 51%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/best-model/special_tokens_map.json (deflated 42%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/best-model/vocab.txt (deflated 53%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/validation_submission.tsv (deflated 90%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/classification_report.txt (deflated 61%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/test_submission.tsv (deflated 90%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/confusion_matrix.png (deflated 20%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/model_info.txt (deflated 30%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/model_performance.png (deflated 27%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/checkpoint-2146/ (stored 0%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/checkpoint-2146/scheduler.pt (deflated 56%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/checkpoint-2146/optimizer.pt (deflated 16%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/checkpoint-2146/config.json (deflated 51%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/checkpoint-2146/rng_state.pth (deflated 25%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/checkpoint-2146/model.safetensors (deflated 7%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/checkpoint-2146/trainer_state.json (deflated 62%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/checkpoint-2146/tokenizer_config.json (deflated 75%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/checkpoint-2146/tokenizer.json (deflated 71%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/checkpoint-2146/training_args.bin (deflated 51%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/checkpoint-2146/special_tokens_map.json (deflated 42%)\n",
      "  adding: kaggle/working/nli_model_20250401_195419/checkpoint-2146/vocab.txt (deflated 53%)\n",
      "Zip file created, you can download it from the Output tab\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='nli_model_20250401_195419.zip' target='_blank'>nli_model_20250401_195419.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/nli_model_20250401_195419.zip"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a zip file of your directory\n",
    "!zip -r /kaggle/working/nli_model_20250401_195419.zip /kaggle/working/nli_model_*\n",
    "\n",
    "print(\"Zip file created, you can download it from the Output tab\")\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'nli_model_20250401_195419.zip')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7019515,
     "sourceId": 11236335,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
